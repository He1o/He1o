---
title: 1. Convolutional Neural Network
date: 2022-06-20
category: Deep learning
---
<!--more-->

# 卷积神经网络

## 1. 历史背景

1950 年代和 1960 年代，Hubel 和 Wiesel 在 的工作表明，猫的视觉皮层包含单独对视野的小区域做出反应的神经元。如果眼睛不动，则视觉刺激影响单个神经元放电的视觉空间区域称为其感受野。相邻细胞具有相似和重叠的感受野。感受野大小和位置在整个皮层系统地变化，以形成完整的视觉空间图。每个半球的皮层代表对侧视野。

1968 年，他们的论文确定了大脑中两种基本的视觉细胞类型：，简单细胞会对
感受野中特定朝向的线段做出反应，而复杂细胞对于特定朝向的线段移动也能做出反应。Hubel 和 Wiesel 还提出了这两种类型的细胞的级联模型，用于模式识别任务。

1980 年，福岛邦彦在此基础上提出了神经认知机模型，引入了卷积层和池化层。卷积层包含的单元的感受野覆盖了前一层的一部分。这种单元的权重向量（一组自适应参数）通常称为滤波器。单元可以共享滤波器。下采样层包含其感受野覆盖先前卷积层块的单元。这样的单元通常计算其补丁中单元激活的平均值。这种下采样有助于正确分类视觉场景中的对象，即使对象发生移动也是如此。

1989 年，Yann LeCun 等人。使用反向传播直接从手写数字的图像中学习卷积核系数。因此，学习是全自动的，比手动系数设计表现更好，并且适用于更广泛的图像识别问题和图像类型。

## 2. 基本原理
### 2.1 卷积运算
在数字信号中，卷积的计算公式为
$$W(T)=\int _{0 }^{T }f(\tau )g(T-\tau )\,\mathrm {d} \tau $$
$f$ 函数是输入信号，$g$ 函数是系统响应。卷积是建立在**输入信号对系统产生的响应是随时间进行累积的**，并且影响的大小与输入的大小具有线性关系。因此，$t$ 时刻进入系统的信号在系统中存在的时间为 $T-t$，由于线性关系，产生的影响即为 $f(t)g(T-t)$，然后对区间 $[0,T]$ 每一时刻的影响进行累加即可，连续情况下也就是积分。

在数字信号中， $g$ 函数是一个递减的函数，即距离时刻 $T$ 越近的输入信号造成的影响也就越大。

在图像处理中，卷积是通过卷积核进行计算的。卷积核矩阵的大小决定了感知野的大小，卷积核中的值决定了图像中相应元素的重要程度，本质上就是加权求和。因此，特定的卷积核可以产生特定的效果，例如相同权重值的卷积核会对输入值进行平均处理，从而对图片进行平滑滤波；而下式中的卷积核则可以对图片进行边缘提取，只有周围像素值差别大的地方例如边缘位置值才不会为 $0$。
$$\begin{bmatrix}
    0 &-1 & 0\\
    -1 &-4 & -1\\
    0 &-1 & 0\\
\end{bmatrix}$$

假设把一张二维的图 $I$ 作为输入，同样有一个二维的核 $K$，卷积的结果为
$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(m,n)K(i-m,j-n)$$
卷积是可交换的，可以等价的写为
$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i-m,j-n)K(m,n)$$

卷积的可交换性在于将核的相对输入进行了翻转，但在神经网络中卷积核并不是给定的，而是随着迭代改变的，因此翻转与否并不重要。在神经网络中实际使用的是互相关函数
$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)$$

卷积核的意义在于稀疏交互 （sparse interactions）、 参数共享（parameter sharing）和等变表示（equivariant representations）

卷积核通过限制输入的作用范围从而达到稀疏连接。如果有 $m$ 个输入和 $n$ 个输出，全连接层的时间复杂度为 $O(m\times n)$，卷积核的大小为 $k$，卷积运算的时间复杂度则为 $O(k\times n)$。尽管同一层输入只对部分输出产生影响，但在深度卷积网络中，深层单元与绝大部分输入具有间接交互。

在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当
它乘以输入的一个元素后就再也不会用到了。而卷积核对于输入都是共享的，因此节省了存储空间。

卷积运算还具有平移等变的特性。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。
https://cs231n.github.io/assets/conv-demo/index.html
### 2.2 池化
在得到卷积特征后，下一步就是用来做分类。理论上，可以用提取到的所有特征训练分类器，但用所有特征的计算量开销会很大，而且也会使分类器倾向于过拟合。

为了解决这个问题，考虑到要描述一个大图像，一个自然的方法是在不同位置处对特征进行汇总统计。例如，可以计算在图像中某一区域的一个特定特征的平均值 (或最大值)，这样概括统计出来的数据，其规模 (相比使用提取到的所有特征) 就低得多，同时也可以改进分类结果 (使模型不易过拟合)。这样的聚集操作称为 “池化”。池化可以保留显著特征，降低特征规模。

- 最大池化函数 (Max Pooling) 给出相邻矩形区域内的最大值
- 平均池化 (Average Pooling) 计算相邻矩形区域内的平均值
- 其他常用的池化函数包括 L2 范数以及基于距中心像素距离的加权平均函数

池化函数具有局部平移不变形。平移的不变性是指当我们对输入进行少量平移时，经过
池化函数后的大多数输出并不会发生改变。当算法关心某个特征是否出现而不是具体位置时，池化就变得尤为有用。

## 3. 卷积层与参数

### 3.1 填充
假设输入图片大小 $(m,n)$，卷积核大小 $(f,f)$，则卷积后的输出图片大小为 $(m-f+1,m-f+1)$。这样会导致卷积运算后输出图片变小，并且图片的角落与边缘像素只参与一次运算，从而会丢失边缘位置的特征信息。

因此在卷积操作前，需要对输入图片进行填充，以增加矩阵大小，通常用 $0$ 来作为填充值。

设每个方向上填充像素点数量为 $p$，填充后的图片大小为 $(m+2p,n+2p)$，卷积核的大小仍为 $(f,f)$，则输出图片大小为 $(m+2p-f+1,n+2p-f+1)$。

填充方法有
- 有效(valid)卷积，不填充，直接卷积，输出大小即为 $(m-f+1,m-f+1)$
- 相同(same)卷积，用 $0$ 填充，使卷积后结果大小与输入一致，$p=(f-1)/2$
- 全(full)卷积，通过填充，使输出大小为 $(m+f-1,n+f-1)$

### 3.2 步幅

除了需要通过填充来避免信息损失，有时也需要通过设置步幅 (Stride) 来压缩一部分信息。步幅表示核在原始图片的水平方向和垂直方向上每次移动的距离。使用卷积步幅，跳过核中的一些位置 (看作对输出的下采样) 来降低计算的开销。如果水平方向和垂直方向的步幅都设为 $s$，则输出尺寸为
$$([\frac{m+2p-f}{s}+1,\frac{n+2p-f}{s}+1])$$

### 3.3 多通道与并行

卷积神经网络中，可以用多组卷积核提取不同的特征，相应的会增加输出的维度，例如输入大小 $(m,n)$，卷积核有两组 $(f,f,2)$，使用 same 卷积，输出则为 $(m,n,2)$。

输入通常也不仅仅是实值的网格，而是由一系列向量的网格，如一幅彩色图像在每一个像素点都会有红绿蓝三种颜色的亮度。当处理图像时，通常把卷积的输入输出都看作是 3 维的张量，其中一个索引用于标明不同的通道 (例如 RGB)，另外两个索引标明在每个通道上的空间坐标。

同样上面的例子，假设输入有三通道，即 $(m,n,3)$，卷积核则为 $(f,f,3,2)$，输出即为 $(m,n,3,2)$。

定义输入通道为 $k$，输出通道为 $l$，则计算公式为
$$S^l(i,j)=\sum_k(I_k*K^l_k)(i,j)$$

## 4. 反向传播
### 4.1 卷积层反向传播
先不考虑多通道，假设输入是矩阵的情况下进行推导。

输入 $\mathbf{x}$，卷积核 $\mathbf{w}$，卷积结果为 $\mathbf{y}$，因此卷积计算为
$$\mathbf{y}=\mathbf{x}*\mathbf{w}$$
每一个元素的计算方法（这里下标 $-1$ 是由于矩阵的角标从 $1$ 开始）
$$y_{i,j}=(\mathbf{x}*\mathbf{w})(i,j)=\sum_m\sum_nx_{i+m-1,j+n-1}w_{m,n}$$
因此偏导数分别为
$$\frac{\partial y_{i,j}}{\partial w_{m,n}}=x_{i+m-1,j+n-1}$$
$$\frac{\partial y_{i-m+1,j-n+1}}{\partial x_{i,j}}=w_{m,n}$$

应用链式法则，得到 $L$ 关于 $\mathbf{x}$ 和 $\mathbf{w}$ 的导数
$$\begin{aligned}
    \frac{\partial L}{\partial x_{i,j}}&=\sum_m\sum_n\frac{\partial L}{\partial y_{i-m+1,j-n+1}}\frac{\partial y_{i-m+1,j-n+1}}{\partial x_{i,j}}\\
    &=\sum_m\sum_n\delta^{(y)}_{i-m+1,j-n+1}w_{m,n}\\
    &=\delta^{(\mathbf{y})}*\mathrm{flip}(\mathbf{w})(i,j)
\end{aligned}$$
其中，$\delta^{(\mathbf{y})}$ 为 $\mathbf{y}$ 的误差矩阵，注意这里为了保证导数矩阵的维度和 $\mathbf{x}$ 一致，需要对 $\delta^{(\mathbf{y})}$ 进行填充，填充后和 $\mathbf{x}$ 一致。

$$\begin{aligned}
    \frac{\partial L}{\partial w_{m,n}}&=\sum_m\sum_n\frac{\partial L}{\partial y_{i,j}}\frac{\partial y_{i,j}}{\partial w_{m,n}}\\
    &=\sum_m\sum_n\delta^{(y)}_{i,j}x_{i+m-1,j+n-1}\\
    &=\delta^{(\mathbf{y})}*\mathbf{x}(m,n)
\end{aligned}$$

经过激活函数，卷积层的输出为
$$\mathbf{a}=g(\mathbf{y})=g(\mathbf{x}*\mathbf{w}+\mathbf{b})$$
因此反向传播为
$$\frac{\partial L}{\partial x_{i,j}}=\delta^{(\mathbf{y})}*\mathrm{flip}(\mathbf{w})\odot g'(\mathbf{y})$$

### 4.2 池化层反向传播
池化函数是一个下采样函数，对于大小为 $m$ 的池化区域，池化函数及导数定义为
- 最大池化：$g(x)=max(x)$，导数为 $\displaystyle\frac{\partial g}{\partial x_i}={\begin{cases}1&x_i=max(x)\\0&\mathrm{otherwise}\end{cases}}$
- 均值池化：$\displaystyle g(x)=\frac{\sum_k x_k}{m}$，导数为 $\displaystyle\frac{\partial g}{\partial x_i}=\frac{1}{m}$
- p 范数池化：$g(x)=\|x\|_p=(\sum_k |x_k|^p)^{1/p}$，导数为$\displaystyle \frac{\partial g}{\partial x_i}=(\sum_k |x_k|^p)^{1/p -1}|x_i|^{p-1}$


定义池化的反向传播为
$$\begin{aligned}
    \frac{\partial L}{\partial x_{(n-1)m-1:nm}} &=\frac{\partial L}{\partial y_n}\frac{\partial y_n}{\partial x_{(n-1)m-1:nm}}\\
    &=\delta_n^{(y)}g_n'\\
    &=upsample(\delta_n^{(y)},g_n')
\end{aligned}$$


上式是一维的上采样（upsample）方式，对于二维大小为 $m*m$ 的池化方式相同。

假设池化大小 $2\times 2$，池化层输出反向传播回来的梯度为
$$\delta_k^{(y)}=\begin{pmatrix}
    2 & 4 \\
    6 & 8
\end{pmatrix}$$
根据输入矩阵的大小，将梯度进行填充
$$\begin{pmatrix}
    0 &0 & 0 &0 \\
    0 &2 & 4 &0 \\
    0 &6 & 8 &0 \\
    0 &0 & 0 &0 \\
\end{pmatrix}$$
如果是最大池化，假设之前在前向传播时记录的最大值位置分别是左上，右上，左下，右下，则转换后的矩阵为
$$\begin{pmatrix}
    2 &0 & 0 &4 \\
    0 &0 & 0 &0 \\
    0 &0 & 0 &0 \\
    6 &0 & 0 &8 \\
\end{pmatrix}$$
如果是均值池化，根据公式可以得到：
$$\begin{pmatrix}
    0.5 &0.5 & 1.0 &1.0 \\
    0.5 &0.5 & 1.0 &1.0 \\
    1.5 &1.5 & 2.0 &2.0 \\
    1.5 &1.5 & 2.0 &2.0 \\
\end{pmatrix}$$

## 参考
> [1. Convolutional neural network - Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)
> [2. 如何通俗易懂地解释卷积？ - zhihu](https://www.zhihu.com/question/22298352/answer/228543288)
> [3. 卷积神经网络的卷积核 - zhihu](https://www.zhihu.com/question/52237725)
> [4. 《深度学习》——IanGoodfellow](https://github.com/MingchaoZhu/DeepLearning/releases/download/v0.0.1/DL_cn.pdf)
> [5. MingchaoZhu  DeepLearning](https://github.com/MingchaoZhu/DeepLearning)
> [6. CS231n: Convolutional Neural Networks for Visual Recognition.](https://cs231n.github.io/convolutional-networks/)
> [7. 从0到1：实现卷积神经网络(实现篇) - zhihu](https://zhuanlan.zhihu.com/p/49205794)
> [8. Notes on Convolutional Neural Networks](https://web-archive.southampton.ac.uk/cogprints.org/5869/1/cnn_tutorial.pdf)
> [9. CSC 411: Lecture 10: Neural Networks](https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/10_nn1.pdf)


