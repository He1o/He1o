---
title: 3. Recurrent Neural Network
date: 2022-07-11
category: Deep learning
---
<!--more-->

# 循环神经网络

## 1. 历史背景
1986 年由 David Rumelhart 提出，也就是方向传播算法提出的时候。


## 2. 循环神经网络模型
循环神经网络是一种处理序列数据的神经网络，例如处理语音和文本。循环神经网络和卷积神经网络相同的一点在于共享权重，如同图片中每个不同的区域实际上使用相同的卷积核进行计算，不同时间段的序列数据传入到同一神经网络层。

循环神经网络与多层感知机的结构更为相似。试想一下，如果用多层感知机对一句话进行标注，假设句子的长度都为 10，那么需要训练 10 个神经网络分别处理不同位置上的单词。而循环神经网络中，不同位置的单词共享一个一个神经网络，也是类似有输入层、隐藏层和输出层，唯一的区别在于，时刻 $t$ 的隐藏层输入 $\mathbf{h}^{(t)}$ 也作为时刻 $t + 1$ 的输入。

循环神经网络具有如下基本设计：
1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络。
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
3. 只有最后一个时间步才有输出，但隐藏单元之间有循环连接的循环网络。

### 2.1 前向传播
我们采用最经典的循环神经网络进行公式推导，即每一个时间步都有输出，且隐藏单元之间有循环连接的循环网络。

时间步 $t$ 的输入 $\mathbf{x}^{(t)}$，隐藏层输出为 $\mathbf{h}^{(t)}$，损失函数为 $L^{(t)}$，因此有
$$L^{(t)}=-\mathbf{y}^T\log\mathrm{softmax}\left(V\tanh(W\mathbf{h}^{(t-1)}+U\mathbf{x}^{(t)}+\mathbf{b})+\mathbf{c}\right)$$

> 可以发现与多层感知机
> $$L=-\sum_{i=1}^{N}\mathbf{y}_i^T\log\mathrm{softmax}\left(W_2\sigma\left(W_1\mathbf{x}_i+\mathbf{b}_1\right)+\mathbf{b}_2\right)$$
> 仅仅多了一项 $W\mathbf{h}^{(t-1)}$

总的损失是每一步损失之和
$$L=\sum_t L^{(t)}$$

定义中间变量
$$\mathbf{a}^{(t)}=W\mathbf{h}^{(t-1)}+U\mathbf{x}^{(t)}+\mathbf{b}$$
$$\mathbf{h}^{(t)}=\tanh(\mathbf{a}^{(t)})$$
$$\boldsymbol{o}^{(t)}=V\mathbf{h}^{(t)}+\mathbf{c}$$

### 2.2 反向传播
我们已经知道
$$\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}=\mathrm{softmax}(\boldsymbol{o}^{(t)})-\mathbf{y}_i$$
不能直接使用链式法则，因此使用复合法则，即微分与导数的联系
$$\begin{aligned}
    \mathrm{d}L&=\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}}^T\mathrm{d}\boldsymbol{o}^{(t)}\right)\\
    &=\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}}^T\mathrm{d}V\mathbf{h}^{(t)}\right)+\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}}^TV\mathrm{d}\mathbf{h}^{(t)}\right)+\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}}^T\mathrm{d}\mathbf{c}\right)
\end{aligned}$$

由第二项得到
$$\frac{\partial L}{\partial \mathbf{h}^{(t)}}={V\frac{\partial L}{\partial \boldsymbol{o}^{(t)}}}^T$$
进一步地利用复合法则
$$\begin{aligned}
    \mathrm{d}L&=\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \mathbf{h}^{(t)}}}^T\mathrm{d}\mathbf{h}^{(t)}\right)\\
    &=\mathrm{tr}\left(\sum_t{\frac{\partial L}{\partial \mathbf{h}^{(t)}}}^T\left(1-(\mathbf{h}^{(t)})^2\right)\odot\mathrm{d}\mathbf{a}^{(t)}\right)\\
    &=\mathrm{tr}\left(\sum_t{ {\frac{\partial L}{\partial \mathbf{h}^{(t)}}}\odot\left(1-(\mathbf{h}^{(t)})^2\right)}^T\mathrm{d}\mathbf{a}^{(t)}\right)
\end{aligned}$$
因此可以得到
$$\frac{\partial L}{\partial W}=\sum_t{ {\frac{\partial L}{\partial \mathbf{h}^{(t)}}}\odot\left(1-(\mathbf{h}^{(t)})^2\right)}{\mathbf{h}^{(t-1)}}^T$$
其他的求导结果与多层感知机类似，从略。

## 3. 长短时记忆网络模型
循环神经网络的一大问题在于梯度消失或梯度爆炸，很难建立长时间间隔的状态依赖关系。长短时记忆网络通过引入门控机制，从而有效解决循环神经网络存在问题。

LSTM 网络引入一个新的内部状态 $\mathrm{c}_t$，专门进行线性的循环信息传递，同时非线性的输出信息给隐藏层的外部状态 $\mathrm{h}_t$，它们具有如下关系
$$\mathbf{c}_t=\boldsymbol{f}_t\odot\mathbf{c}_{t-1}+\boldsymbol{i}_t\odot \tilde{\mathbf{c}}_t$$
$$\mathbf{h}_t=\boldsymbol{o}_t\odot \tanh(\mathbf{c}_t)$$
其中 $\boldsymbol{f}_t,\boldsymbol{i}_t,\boldsymbol{o}_t\in[0,1]$ 是三个门来控制信息传递的路径。$\tilde{\mathbf{c}}_t$ 是通过非线性函数得到的候选状态
$$\tilde{\mathbf{c}}_t=\tanh(W_c\mathbf{x}_t+U_c\mathbf{h}_{t-1}+\mathbf{b}_c)$$
在每个时刻，LSTM 网络内部状态 $\mathbf{c}_t$ 记录了到当前时刻为止的历史信息。

- 遗忘门 $\boldsymbol{f}_t$ 控制上一时刻的内部状态 $\mathbf{c}_{t-1}$ 需要遗忘多少信息。
- 输入门 $\boldsymbol{i}_t$ 控制当前时刻的候选状态 $\tilde{\mathbf{c}}_t$ 有多少信息需要保存。
- 输出门 $\boldsymbol{o}_t$ 控制当前时刻的内部状态 $\mathbf{c}_{t}$ 有多少信息需要输出给外部状态 $\mathbf{h}_t$。
  
三个门的计算方式为
$$\boldsymbol{f}_t=\sigma (W_f\mathbf{x}_t+U_f\mathbf{h}_{t-1}+\mathbf{b}_f)$$
$$\boldsymbol{i}_t=\sigma (W_i\mathbf{x}_t+U_i\mathbf{h}_{t-1}+\mathbf{b}_i)$$
$$\boldsymbol{o}_t=\sigma (W_o\mathbf{x}_t+U_o\mathbf{h}_{t-1}+\mathbf{b}_o)$$

给出了 LSTM 网络的循环单元结构，其计算过程为：
1. 首先利用上一时刻的外部状态 $\mathbf{h}_{t-1}$ 和当前时刻的输入 $\mathbf{x}_t$，计算出三个门，以及候选状态 $\tilde{\mathbf{c}}_t$；
2. 结合遗忘门 $\boldsymbol{f}_t$ 和输入门 $\boldsymbol{i}_t$ 来更新记忆单元 $\mathbf{c}_t$；
3. 结合输出门 $\boldsymbol{o}_t$，将内部状态的信息传递给外部状态 $\mathbf{h}_{t}$

循环神经网络中的隐状态 𝒉 存储了历史信息，可以看作是一种记忆（Memory）. 在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作是一种短期记忆（Short-Term Memory）. 在神经网络中，长期记忆（Long-Term Memory）可以看作是网络参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆. 而在 LSTM 网络中，记忆单元 𝒄 可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔. 记忆单元 𝒄 中保存信息的生命周期要长于短期记忆 𝒉，但又远远短于长期记忆，因此称为长短期记忆（Long Short-Term Memory）