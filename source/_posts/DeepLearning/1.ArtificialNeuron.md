---
title: 1. Artificial Neuron
date: 2022-06-01
category: Deep Learning
---
<!--more-->

# 神经元

## 1 激活函数
### 1.1 sigmoid 型函数

sigmoid 型函数指 S 型曲线函数，为两端饱和函数。饱和指的是 $x\rightarrow \infty$ 时，其导数 $f'(x)\rightarrow 0$

**sigmoid 函数**

该函数将取值为 $\left(-\infty,+\infty\right)$ 的数映射到 $\left(0,1\right)$ 之间。sigmoid 的公式为 
$$g\left(z\right)=\sigma(z)=\frac{1}{1+e^{-z}}$$
导函数为
$$g'\left(z\right)=g\left(z\right)\left(1-g\left(z\right)\right)$$
sigmoid函数存在的问题在于
1. 需要计算指数，计算时间花费多。
2. 当取值非常大或非常小时，sigmoid 的导数接近于 0，有可能会产生梯度消失。
3. 函数的输出值不是以 0 为均值，会使得其后一层的神经元的输入发生偏置偏移(Bias Shift)，并进一步使得梯度下降的收敛速度变慢，不利于下层的计算。因此，最好不用于隐藏层，可以作为输出层使用。

sigmoid函数的输出可以看作是概率分布，使得神经网络可以更好地和统计学习模型进行结合。

**tanh 函数**

将取值为 $\left(-\infty,+\infty\right)$ 的数映射到 $\left(-1,1\right)$ 之间。tanh 的公式为
$$g\left(z\right)=\text{tanh}=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$
tanh 函数的导函数为
$$g'\left(z\right)=1-g\left(z\right)^2$$
tanh导数具有较大的值域，可以缓解梯度消失的情况

tanh 函数均值为 0，即零中心化，弥补了 sigmoid 函数均值为 $0.5$ 的缺点。但由于 $g'\left(z\right)$ 在两端取值接近于 0，因此也会有梯度消失的情况发生。


**Hard-Logistic 函数**

sigmoid 和 tanh 函数计算开销大。这两个函数都是在中间(0 附近)近似线性，两端饱和，因此，这两个函数可以通过分段函数来近似。

sigmoid 在 0 附近的一阶泰勒展开为

$$\begin{aligned}
    g_l(x)&\thickapprox \sigma(0)+x\cdot \sigma'(0)\\
    &= 0.25x+0.5
\end{aligned}$$

通过一阶泰勒对0附近的函数进行近似
$$\begin{aligned}
    \text{hard-logistic(x)}&=\begin{cases}
        1&g_l(x)\geq 1\\
        g_l&0<g_l(x)<1\\
        0&g_l(x)\le 0
    \end{cases}\\
    &= \max(\min(0.25x+0.5,1),0)
\end{aligned}$$


**Hard-Tanh 函数**

tanh 函数在 0 附近痾一阶泰勒展开为
$$\begin{aligned}
    g_t(x)&\thickapprox \tanh(0)+x\cdot\tanh'(0) \\
    &=x
\end{aligned}$$

分段近似
$$\begin{aligned}
    \text{hard-tanh(x)}&=\max(\min(g_t(x),1),-1)\\
    &= \max(\min(x,1),-1)
\end{aligned}$$


### 1.2 ReLU 函数

**ReLU 函数**

ReLU 函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了 sigmoid 函数以及 tanh 函数的梯度消失问题。

ReLU 函数的公式为：
$${\displaystyle 
g\left(z\right)={
    \begin{cases}
        z&z\geq 0\\
        0&z<0
    \end{cases}
    }
}
$$
ReLU 函数的导函数为
$${\displaystyle 
g‘\left(z\right)={
    \begin{cases}
        1&z\geq 0\\
        0&z<0
    \end{cases}
    }
}
$$
ReLU 函数的优点在于当输入为正数的时候，不存在梯度消失的问题。同时，由于其线性关系，计算速度会快许多。ReLU 函数的缺点是当输入为负时，仍然会有梯度消失的情况发生。

ReLU 函数被认为有生物上的解释性，比如单侧抑制、宽兴奋边界(即兴奋程度也 可以非常高)。 在生物神经网络中，同时处于兴奋状态的神经元非常稀疏。 人脑中，在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态。 Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 ReLU 却具有很好的稀疏性，大约 50% 的神经元会处于激活状态。


ReLU 函数的缺点在于输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。此外，ReLU 神经元在训练时比较容易“死亡”。 在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活. 这种现象称为死亡ReLU问题(DyingReLU Problem)，并且也有可能会发生在其他隐藏层。

**Leaky ReLU 函数**

公式为
$$\begin{aligned}
    g\left(z\right)&={
       \begin{cases}
           z&z\geq 0\\
           az&z<0
       \end{cases}
    }\\
    &= \max(0,z)+a \min(0,z)
\end{aligned}
$$
其中，$a$ 的取值在 $\left(0,1\right)$ 之间。

导函数为
$${\displaystyle 
g\left(z\right)={
    \begin{cases}
        1&z\geq 0\\
        a&z<0
    \end{cases}
    }
}
$$
Leaky ReLU 函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题，当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活。

$\gamma$ 是一个很小的常数，例如 $0.01$，当 $\gamma < 1$ 时，Leaky ReLU 函数也可以写为
$$g(z)=\max(x,\gamma x)$$
相当于是一个比较简单的 maxout 单元。

**Parametric ReLU**

带参数的 ReLU 引入一个可学习的参数，不同神经元可以有不同的参数，
$$\begin{aligned}
    PReLU_i\left(z\right)&={
       \begin{cases}
           z&z\geq 0\\
           az&z<0
       \end{cases}
    }\\
    &= \max(0,z)+\gamma_i \min(0,z)
\end{aligned}
$$
PReLU可以允许不同神经元具有不同的参数，也可以一组神经元共享一 个参数

**ELU 函数**
ELU(Exponential Linear Unit，指数线性单元)是一个近似的零中心化的非线性函数，其定义为
$$\begin{aligned}
    ELU\left(z\right)&={
       \begin{cases}
           z&z\geq 0\\
           \gamma(\exp(z)-1)&z<0
       \end{cases}
    }\\
    &= \max(0,z)+\min(0, \gamma(\exp(x)-1))
\end{aligned}
$$

其中 $\gamma\ge 0$ 是一个超参数，决定 $x\le 0$ 时的饱和曲线，并调整输出均值在 $0$ 附近。

**Softplus 函数**
Softplus 函数可以看作是 ReLU 函数的平滑版本，定义为
$$Softplus(x)=\log(1+\exp(x))$$
Softplus函数其导数刚好是Logistic函数。 Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，却没有稀疏激活性。

### 1.3 Swish 函数

Swish 函数是一个自门控激活函数，定义为
$$swish(x)=x\sigma(\beta x)$$

$\sigma$ 为 Logistic 函数，$\beta$ 为可学习的参数或一个固定超参数。$\sigma$ 可以看作一种软性门控机制。当 $\sigma(\beta x)$ 接近于 1 时，门处于开的状态，激活函数输出近似于 $x$ 本身，当接近于 0 时，门处于关的状态，激活函数输出近似于 0。

当 $\beta=0$ 时，Swish 函数变成线性函数 $x/2$。当 $\beta=1$ 时，Swish 函数在 $x>0$ 时近似线性，在 $x<0$ 时近似饱和，同时具有一定非单调性。 当 $\beta \rightarrow \infty$ 时，$\sigma$ 趋向于离散的 0-1 函数，Swish 函数近似于 ReLU 函数。因此，Swish 函数可以看作线性函数和 ReLU 函数之间的非线性插值函数，其程度由参数 $\beta$ 控制。

### 1.4 GELU 函数
高斯误差线性单元(Gaussian Error Linear Unit，GELU) 和 Swish 函数相类似，也是通过门控机制来调整其输出值的激活函数
$$GELU=xP(X\le x)$$
其中 $P(X\le x)$ 是高斯分布 $\mathcal{N}(\mu, \sigma^2)$ 的累积分布函数，其中 $\mu, \sigma$ 一般取 0 和 1。由于高斯分布的累积分布函数为 S 型函数，因此 GELU 可以用 Tanh 函数或 Logistic 函数来近似
$$GELU\approx0.5x\left(1+\tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3))\right)$$
$$GELU\approx x\sigma(1.702x)$$

当用 Logistic 函数来近似时，相当于一种特殊的 Swish 函数。

### 1.5 Maxout 单元
Maxout 单元也是一种分段线性函数。以上其他的激活函数输入是神经元的净输入 $z$，是一个标量。而 Maxout 单元的输入是上一层神经元的全部原始输出，是一个向量。

Maxout 单元的非线性函数定义为
$$maxout(\mathbf{x})=\max_k (z_k)$$

Maxout 单元不单是净输入到输出之间的非线性映射，而是整体学习输入到输出之间的非线性映射关系。 Maxout激活函数可以看作任意凸函数的分段线性近似，并且在有限的点上是不可微的。

### 1.6 Sfotmax 函数

softmax 函数通常被认为是多个 sigmoid 的组合。当神经网络用于二分类问题时，只需要 sigmoid 输出一个区间在 $[0,1]$ 的值即可。当用于多分类问题时，输出层的神经元数量和类别总数量是一样的，通过 softmax 函数输出每一个类别的概率。
$$ g\left(\mathbf{z}\right)_j=\frac{e^{z_j}}{\displaystyle \sum^K_{k=1}e^{z_k}} \qquad j=1,2,\dots,K$$
求导函数需要分情况讨论，当 $z_i=z_j$ 时
$$\begin{aligned}
    g'\left(z_i=z_j\right)&=\frac{e^{z_j}\sum^K_{k=1}e^{z_k}-\left(e^{z_j}\right)^2}{\left(\sum^K_{k=1}e^{z_k}\right)^2} \\
    &= g\left(\mathbf{z}\right)_j-g\left(\mathbf{z}\right)_j^2 \\
    &=g\left(\mathbf{z}\right)_j\left(1-g\left(\mathbf{z}\right)_j\right)
\end{aligned}$$
当 $z_i\ne z_j$ 时
$$\begin{aligned}
    g'\left(z_i\ne z_j\right)&=\frac{0-e^{z_j}e^{z_i}}{\left(\sum^K_{k=1}e^{z_k}\right)^2} \\
    &= -g\left(\mathbf{z}\right)_j\cdot g\left(\mathbf{z}\right)_i \\
\end{aligned}$$
因此偏导数的最终表达式为
$${\displaystyle 
\frac{\partial g\left(\mathbf{z}\right)_j}{\partial z_i}={
    \begin{cases}
        g\left(\mathbf{z}\right)_j\left(1-g\left(\mathbf{z}\right)_j\right)&z_i=z_j\\
        -g\left(\mathbf{z}\right)_j\cdot g\left(\mathbf{z}\right)_i&z_i\ne z_j
    \end{cases}
    }}
$$


### 2.2 损失函数
损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

平方损失函数
$$L = \sum_k\frac{1}{2}\left(r_k-y_k\right)^2$$

均方误差是与单位高斯分布的输出相关联的交叉熵损失，也就是高斯分布最大似然函数的相反数。

交叉熵损失函数
$$L = -\sum_kr_k\ln y_k$$

交叉熵函数实际上是多项式分布的最大似然函数加上一个负号，将最大值问题转换为损失值最小的问题，本质是一样的。在多分类问题中，输出是 softmax 函数，如同逻辑回归一样。

假设 softmax 函数的输入是 $z_i$，推导一下交叉熵损失函数的偏导数
$$\frac{\partial L}{\partial z_i}=-\sum_k\frac{r_k}{g\left(\mathbf{z}\right)_k}\frac{\partial g\left(\mathbf{z}\right)_k}{\partial z_i}$$
分为 $k=i$ 与 $k\ne i$ 进行求和
$$\begin{aligned}
    \frac{\partial L}{\partial z_i}&=\sum_{k\ne i}\frac{r_k}{g\left(\mathbf{z}\right)_k}g\left(\mathbf{z}\right)_k\cdot g\left(\mathbf{z}\right)_i - \frac{r_i}{g\left(\mathbf{z}\right)_i}g\left(\mathbf{z}\right)_i\left(1-g\left(\mathbf{z}\right)_i\right)\\
    &=\sum_{k\ne i}r_kg\left(\mathbf{z}\right)_i+r_ig\left(\mathbf{z}\right)_i-r_i\\
    &=g\left(\mathbf{z}\right)_i\sum_k r_k- r_i
\end{aligned}$$
多分类问题都采用 one-hot 编码，只有一个类别的标签值为 $1$，其余为 $0$，因此
$$\sum_k r_k=1$$
最终得到
$$\frac{\partial L}{\partial z_i}=g\left(\mathbf{z}\right)_i-r_i$$



## 参考
> [1. Artificial neural network - Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)
> [2. Backpropagation - Wikipedia](https://en.wikipedia.org/wiki/Backpropagation)
> [3. 深度学习中【激活函数】存在的意义是什么？](https://zhuanlan.zhihu.com/p/80730031)
> [4. 深度学习基础——激活函数以及什么时候该使用激活函数](https://www.datalearner.com/blog/1051508750742453)
> [5. Fundamentals of Deep Learning – Activation Functions and When to Use Them?](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)
> [6. 一文详解Softmax函数 - zhihu](https://zhuanlan.zhihu.com/p/105722023)
> [7. 常见的损失函数(loss function)总结 - zhihu](https://zhuanlan.zhihu.com/p/58883095)
> [8. CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes03-neuralnets.pdf\right)
> [9. CSC 411: Lecture 10: Neural Networks](https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/10_nn1.pdf)


