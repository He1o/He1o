---
title: 5. Decision Tree
date: 2021-12-22
category: Machine learning
---
<!--more-->


# 决策树

## 1. 历史背景

关于树形结构的历史可以追溯到古巴比伦，在这里就不过多阐述。

1963：最早的发表在文献中的回归树算法是Automatic Interaction Detection(AID, Morgan & Sonquist)。AID 从根节点开始递归的将数据拆分为两个子节点，选择划分特征的依据是通过类似于方差的公式 $\sum (X-\overline{X})^2$ ，称为不纯函数，计算划分子节点的误差平方和使其越小说明划分效果越好，因为 $\sum (X-\overline{X})^2=\sum X^2-N\overline{X}^2$，所以最终只要计算划分后 $\sum N_i\overline{X_i}^2$ 即可，选择最大的特征进行划分。当划分后的误差平方减少值小于 $0.02(\sum X^2-N\overline{X}^2)$ 则结束算法。

1972：THeta Automatic Interaction Detection (THAID, Messenger & Mandell) 将上述思想应用到分类问题中，提出了第一个分类树，并用熵函数和基尼指数代替上述不纯函数。

1980：CHi-squared Automatic Interaction Detector (CHAID) 由 Kass 创建，通过 $\mathcal{X}^2$ 拟合优度检验来找到主要特征，假设有两个类别，默认分布概率都是 $1/2$，计算 $\mathcal{X}^2=\sum\sqrt{(n_i-np_i)/(np_i)}$，选取最大的特征进行分割。卡方拟合优度越大意味着观察频数与默认均匀分布的区别越大，也就是越趋于有序，当同一特征将样本均归于同一类时，卡方拟合优度达到最大，当两个类别各有一半时，卡方拟合优度则为0。

1984：Classification And Regression Trees (CART) 伯克利的统计学教授 Leo Breiman 和 Charles Stone 以及斯坦福的 Jerome Friedman 和 Richard Olshen 共同建立。它同样使用 AID 的贪婪搜索方法，但增加了新颖的改进。CART 不再使用停止规则，而是生成一颗大树，通过最低交叉验证对树结构进行剪枝。这解决了 AID 和 THAID 的欠拟合和过拟合问题。

1986：John Ross Quinlan 提出了一个新概念有多个答案的树，而CART 和所有其他算法对每个问题只有两个答案（称为二叉树）。Quinlan 使用称为增益比的杂质标准发明了  Iterative Dichotomiser 3（ID3）。

1993：C4.5 是 ID3 算法的扩展，C4.5 解决了其前身的缺点，在数据挖掘杰出论文的 Top 10 算法中排名第一（Springer LNCS，2008）。

## 2.决策树结构
决策树模型是一种对实例进行分类或回归的树形结构。树结构的每个节点均代表输入特征空间的部分子区域，对于不再分割的节点称为终端节点（terminal node）也叫叶节点（leaf node），在图中由矩形框表示；除了叶节点以外的称为内部节点（internal node），在图中用圆形表示。

构建决策树的时候，从根节点开始，通过一定规则选定分割的特征维度和特征值，然后构建下一层的多个节点。特别地，如果每一个节点只通过一个特征的一个值分割为两部分，该决策树则称为二叉决策树。

对于决策树中的每一个节点的后代节点都是不相交的，也就是它们之间互斥，同时一个节点所有后代子集的并集即为该节点。例如图1中所示，$X_2$ 与 $X_3$ 互斥且 $X=X_2\bigcup X_3$。决策树中的每一层中的节点的并集构成整体输入特征空间。

终端节点是输入空间的一个子区域，所有终端节点互斥且完备，即所有终端节点的并集构成整体输入空间。在分类问题中，每个终端节点由一个类标签指定，可能有多个终端节点有相同的类标签。分类器最终分类划分是将同一类对应的所有终端子集放在一起得到的。例如，上图的划分结果为
$$
\begin{aligned}
    A_1&=X_{15} \\
    A_2&=X_{10}\bigcup X_{16}\\
    A_3&=X_{11}\bigcup X_{14}\\
    A_4&=X_{6}\bigcup X_{17}\\
    A_5&=X_8 \\
    A_6&=X_{12} \\

\end{aligned}
$$

综上所述，可以发现构建决策树需要解决三个问题：
1. 在每个中间节点选择拆分的方法
2. 终止条件
3. 每个终端叶子节点的输出结果

进一步地，如果想要决策树模型不仅对训练数据有很好的拟合效果，同时对未知数据有很好的预测，也就是避免过拟合现象的发生，我们还可能需要对已生成的决策树进行自下而上的剪枝，使树结构变得更简单，从而使它具有更好的泛化能力，也就是

4. 决策树的剪枝 

有关第一个问题，在这里先定义一个通用的方法和标准。

定义训练集的输入实例数为 $N$，每个类别的数量为 $N_j$，则每个节点 $t$ 中的某一类别的数量为 $N_j(t)$，落入节点的概率为 $p(t) = N(t)/N$，条件概率$p(t|j)=N_j(t)/N_j$，属于 $j$ 类且落入节点 $t$ 两事件同时发生的概率为 $p(j,t)=p(t|j)p(j)$，在这里我们先假设先验概率 $p(j)$ 就是训练集中的类别概率 $p(j)=N_j/N$。因此，根据贝叶斯公式我们可以得到后验概率
$$
\begin{aligned}
    P(j|t)&=\frac{p(t|j)p(j)}{p(t)}\\
    &=\frac{(N_j(t)/N_j) \ast (N_j/N)}{N(t)/N}\\
    &=\frac{N_j(t)}{N(t)}
\end{aligned}
$$
> 忙活半天，得到了一个很显而易见的结果 -_-!  但这个显而易见的比例实际上是条件概率。

通常情况下先验概率 $p(j)$ 被视为训练集中的比例 $N_j/N$。但学习样本中的比例无法反映到现实中实际的比例，就例如评价道路的好坏，实际上得到的训练集都是通过 bad case 中的路线，实际上，可能差路线存在的比例会更低。

因此，我们由其他途径的得到先验概率 $\pi(j)$ 取代训练集中的概率 $p(j)$，联合概率即为 
$$p(j,t)=\pi(j)N_j(t)/N_j$$
同时任何案例落入节点 $t$ 的概率重新带入估计 
$$p(t)=\sum_jp(j,t)$$
最后再去计算条件概率 $p(j|t)$，可以发现，替换后的条件概率依旧满足 
$$\sum_jp(j|t)=1$$

决策树划分的目的就是希望能通过特征将实例的类别进行区分出来。决策树的每个内部节点将分割为更多的子区域，每个子区域的条件是一样的，而所希望的就是某一类的条件概率 $P(j|t)$ 尽可能的大。换句话说，希望每个后代节点中的数据比父节点中的数据更“纯”（purer）。

例如，假设有个六分类问题，根节点的条件概率 $(p_1,p_2,p_3,p_4,p_5,p_6)$ 为 $(1/6,1/6,1/6,1/6,1/6,1/6)$，一个好的划分结果就是两个子节点的条件概率也就是类别比例为 $(1/3,1/3,1/3,0,0,0)$ 和 $(0,0,0,1/3,1/3,1/3)$ 。

因此我们需要寻找一个测度去衡量一个节点中子集的不纯度（impurity），定义一个非负函数，要求当所有类均匀混合在一起时它的不纯度最大，当节点中只包含一个类时，该值最小，现将这个函数定义为 $i(t)$。

对于任何中间节点 $t$，假设有一个节点的候选切分 $s$ 将节点分割为 $t_L$ 和 $t_R$，这样 $t$ 中的事件（case）分别进入 $t_L$ 和 $t_R$ 的比例为 $p_L$ 和 $p_R$，那么最终切分点 $s$ 的好坏定义为不纯度的好坏的减少
$$\triangle i(s,t)=i(t)-p_Li(t_L)-p_Ri(t_R)$$

对于不是二叉树而是多切分节点的算法只需将上公式扩展为多个即可，但始终 $p_1+p_2+\cdots+p_n=1$。



至此，我们得到一个通用的标准去衡量切分的好坏程度，在不同的算法中以及回归和分类树中，所不同的只是不纯度函数 $i(t)$ 的不同。

## 3. CART 算法

CART 算法由 Breiman 等人在 1984 年提出。CART 同样由特征选择、树的生成以及剪枝组成，既可用于分类也可以用于回归。

CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 是二叉树，递归的二分每个特征，分为“是”或“否”，将输入空间即特征空间划分为有限个单元，并在每个单元上确定预测的概率分布。

CART回归树实际是基于AID的决策树方法，但增加了最优切分点的寻找以及剪枝的过程。CART分类树使用到的基尼指数也不是第一次出现。
### 3.1 CART 分类
我们从前述四个问题出发，先解决最直观的第三个问题

3. 每个终端叶子节点的输出结果

**定义1** 一个决策树的所有终端节点的集合为 $T$，每个终端节点 $t\in T$，所有分类标签的集合为 $j\in \{1,2,\cdots,J\}$，则某一节点所分配的类为 $j(t)$。

> $j(t)$ 是类分配结果同时也代表一种类分配规则

对于任何一种类分配结果 $j(t)$，如果实例落入节点 $t$，则错误分类概率的重新替换估计（resubstitution estimate）为
$$\sum_{j\ne j(t)}p(j|t)$$
我们需要最小化这个误分类估计的规则作为我们类分配规则 $j^*(t)$。

由于一个节点最终只能输出一个类别，一个直观的类分配规则就是将某一节点中占比最大的类分配给该节点，这样做的另一个原因就是使误分类的概率最小。

**定义2** 类分配规则 $j^\ast(t)$：如果 $\displaystyle p(j|t)=\max_i p(i|t)$，则 $j^\ast(t)=j$，如果两个或多个不同类别达到最大值，则将 $j^\ast(t)$ 任意指定为任何一个最大化类别。

第三个问题得到解决。

进一步地，我们就可以得到节点 $t$ 的误分类概率的重新带入估计 $r(t)$ 为
$$
\begin{aligned}
    r(t)&=\sum_{j\ne j^*(t)}p(j|t)\\
    &=1-\max_j p(j|t)
\end{aligned}
$$

整体决策树误分类率的重新代入估计则为 
$$R(T)=\sum_{t\in T}r(t)p(t)$$

> 以上公式的推导是在假定所有错误分类为 $i$ 类对象的成本或损失都是一样的，在某些问题中希望能将它们区分出来，例如某一类错误分类的代价将会更大，也就是希望尽可能减少其误分类的概率。因此，引入一组错误分类成本 $c(i|j)$ 代表将 $j$ 类对象误分类为 $i$ 类对象的代价，它应该满足
> $${\displaystyle c(i|j)={\begin{cases}C_i(C_i\ge 0)&i\ne j\\0&i=j\end{cases}}}$$
> 随机一个实例落入节点 $t$ 并被分类为类别 $i$，则估计的预期误分类代价为 
> $$\sum_j c(i|j)p(j|t)$$
> 一个自然地节点分配规则是选择 $i$ 来最小化这个表达式，因此，
> 令 $j^*(t)=i_0$，


### 3.2 CART 回归
回归树需要解决三个问题


训练数据集：
$$T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}$$

第三个问题最直观也最好解决，假设输出值的函数为 $d$，总的均方误差为
$$R(d)=\frac{1}{N}\sum_n^N(y_n-d(\mathbf x_n))^2$$

对于每个叶子节点，假设输入空间为 $t$，根据最小二乘法，为了使 $R(d)$ 最小，输出值应为

$$d_t(\mathbf x_n)=\overline{y}_t=\frac{1}{N_t}\sum_{\mathbf x_n\in t}y_n$$

**即回归树建立完后，每个叶子节点的输出值为其分配样本的均值。**

进一步的，将预测值带入每个节点 $t$，并用 $R(T)$ 取代 $R(d)$

$$R(T)=\frac{1}{N}\sum_{t\in T}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$$

定义
$$R(t)=\frac{1}{N}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$$

> 注意这里不是 $N_t$ 

因此，$R(T)$ 还可以写为
$$R(T)=\sum_{t\in T}R(t)$$

对以上表达式一个简单的解释就是，对于每个节点 $t$，$\displaystyle \sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$ 是节点内的误差平方和，对 $t$ 求和给出所有节点的总平方和，再除以 $N$ 给出平均值。

假定分割前的均方误差为 $R(t)$，分割点为特征向量 $j$，分割值为 $s$，输入空间 $t$ 将被分为两个区域
$$t_L=\{x|x^{(j)}\le s\},t_R=\{x|x^{(j)}\ge s\}$$

为了使分割更为有效，将使均方误差减少的尽可能多，因此目标函数为
$$\triangle R(s,t)=R(t)-R(t_L)-R(t_R)$$

与 $s$ 相关的变量只有后两项，因此
$$\max \triangle R(s,t)=\min [R(t_L)+R(t_R)]$$

令 $p(t)=\frac{N_t}{N}$ 表示随机选择输入变量落入节点 $t$ 的概率，定义节点内方差
$$s^2(t)=\frac{1}{N_t}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$$

因此可得 $R(t)=s^2(t)p(t)$，并且
$$R(T)=\sum_{t\in T}s^2(t)p(t)$$

$t$ 的最佳分割也就是使加权方差最小化
$$\min[p(t_L)s^2(t_L)+p(t_R)s^2(t_R)]$$

实际上概率 $p$ 可以带入进去，再乘以 $N$，就得到李航书中的公式

$$\min_{j,s} \left[\sum_{x_i \in t_L} (y_i-\overline{y}_{t_L})^2+ \sum_{x_i \in t_R} (y_i-\overline{y}_{t_R})^2\right]$$

依次选取切分变量 $j$，遍历空间中所有输入点确定最优切分值 $s$，找到最优切分变量和切分值使上式最小。

对每个区域依次重复上述过程，直到达到终止条件。与 AID 不同，CART 终止条件为节点中的样本个数小于特定阈值（通常为5），或者节点中样本均为一个类别，即该节点为纯节点。

至此回归的三个问题均已解决，归纳如下


https://holypython.com/dt/decision-tree-history/
https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/
https://en.wikipedia.org/wiki/C4.5_algorithm
https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/
https://www.cnblogs.com/fushengweixie/p/8039991.html