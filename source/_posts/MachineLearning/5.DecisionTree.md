---
title: 5. Decision Tree
date: 2021-12-22
category: Machine learning
---
<!--more-->


# 决策树

## 1. 历史背景

关于树形结构的历史可以追溯到古巴比伦，在这里就不过多阐述。

1963：最早的发表在文献中的回归树算法是Automatic Interaction Detection(AID, Morgan & Sonquist)。AID 从根节点开始递归的将数据拆分为两个子节点，选择划分特征的依据是通过类似于方差的公式 $\sum (X-\overline{X})^2$ ，称为不纯函数，计算划分子节点的误差平方和使其越小说明划分效果越好，因为 $\sum (X-\overline{X})^2=\sum X^2-N\overline{X}^2$，所以最终只要计算划分后 $\sum N_i\overline{X_i}^2$ 即可，选择最大的特征进行划分。当划分后的误差平方减少值小于 $0.02(\sum X^2-N\overline{X}^2)$ 则结束算法。

1972：THeta Automatic Interaction Detection (THAID, Messenger & Mandell) 将上述思想应用到分类问题中，提出了第一个分类树，并用熵函数和基尼指数代替上述不纯函数。

1980：CHi-squared Automatic Interaction Detector (CHAID) 由 Kass 创建，通过 $\mathcal{X}^2$ 拟合优度检验来找到主要特征，假设有两个类别，默认分布概率都是 $1/2$，计算 $\mathcal{X}^2=\sum\sqrt{(n_i-np_i)/(np_i)}$，选取最大的特征进行分割。卡方拟合优度越大意味着观察频数与默认均匀分布的区别越大，也就是越趋于有序，当同一特征将样本均归于同一类时，卡方拟合优度达到最大，当两个类别各有一半时，卡方拟合优度则为0。

1984：Classification And Regression Trees (CART) 伯克利的统计学教授 Leo Breiman 和 Charles Stone 以及斯坦福的 Jerome Friedman 和 Richard Olshen 共同建立。它同样使用 AID 的贪婪搜索方法，但增加了新颖的改进。CART 不再使用停止规则，而是生成一颗大树，通过最低交叉验证对树结构进行剪枝。这解决了 AID 和 THAID 的欠拟合和过拟合问题。

1986：John Ross Quinlan 提出了一个新概念有多个答案的树，而CART 和所有其他算法对每个问题只有两个答案（称为二叉树）。Quinlan 使用称为增益比的杂质标准发明了  Iterative Dichotomiser 3（ID3）。

1993：C4.5 是 ID3 算法的扩展，C4.5 解决了其前身的缺点，在数据挖掘杰出论文的 Top 10 算法中排名第一（Springer LNCS，2008）。

## 2. CART 算法

CART 算法由 Breiman 等人在 1984 年提出。CART 同样由特征选择、树的生成以及剪枝组成，既可用于分类也可以用于回归。

CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 是二叉树，递归的二分每个特征，分为“是”或“否”，将输入空间即特征空间划分为有限个单元，并在每个单元上确定预测的概率分布。

CART回归树实际是基于AID的决策树方法，但增加了最优切分点的寻找以及剪枝的过程。CART分类树使用到的基尼指数也不是第一次出现。

### 2.1 CART 回归
回归树需要解决三个问题
1. 在每个中间节点选择分裂的方法
2. 终止条件
3. 每个终端叶子节点的输出值

训练数据集：
$$T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}$$

第三个问题最直观也最好解决，假设输出值的函数为 $d$，总的均方误差为
$$R(d)=\frac{1}{N}\sum_n^N(y_n-d(\mathbf x_n))^2$$

对于每个叶子节点，假设输入空间为 $t$，根据最小二乘法，为了使 $R(d)$ 最小，输出值应为

$$d_t(\mathbf x_n)=\overline{y}_t=\frac{1}{N_t}\sum_{\mathbf x_n\in t}y_n$$

**即回归树建立完后，每个叶子节点的输出值为其分配样本的均值。**

进一步的，将预测值带入每个节点 $t$，并用 $R(T)$ 取代 $R(d)$

$$R(T)=\frac{1}{N}\sum_{t\in T}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$$

定义
$$R(t)=\frac{1}{N}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$$

> 注意这里不是 $N_t$ 

因此，$R(T)$ 还可以写为
$$R(T)=\sum_{t\in T}R(t)$$

对以上表达式一个简单的解释就是，对于每个节点 $t$，$\displaystyle \sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$ 是节点内的误差平方和，对 $t$ 求和给出所有节点的总平方和，再除以 $N$ 给出平均值。

假定分割前的均方误差为 $R(t)$，分割点为


https://holypython.com/dt/decision-tree-history/
https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/
https://en.wikipedia.org/wiki/C4.5_algorithm
https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/