---
title: 10. Hidden Markov Models
date: 2022-04-22
category: Machine learning
---
<!--more-->
# 隐马尔科夫模型
## 1. 历史背景

## 2. 问题引入
假设有一个由多个可观测的样本组合的序列 $\mathbf{y}=(y_1,y_2,\dots,y_T)$，我们称之为观测序列。观测序列中的 $t$ 可以指时间、染色体或 DNA 上位点的索引或单词在句子中的位置，对于每个 $t\in1,2,\dots,T$，我们希望得到一个对应的感兴趣的隐藏状态变量 $x_t\in \mathcal{X}$，其中 $\mathcal{X}$ 是有限的集合，由 $x_t$ 组成的序列称为状态序列 $\mathbf{x}$。例如在词性标注中，观测序列就是文档中的 $t$ 个单词，状态序列就是每个单词对应的词性标签。

试想一下，如果用之前的机器学习方法可以怎么做？

可以用贝叶斯估计，最大似然条件概率 $\displaystyle\argmax \frac{p(Y=\mathbf{y}|X=\mathbf{x})p(X=\mathbf{x})}{p(Y=\mathbf{y})}$，显然单词间的组合太多了，需要的样本量之多几乎不可能做到，只能利用朴素贝叶斯的思想，假定 $(x_i,y_i)$ 与 $(x_j,y_j)$ 相互独立，这样我们就可以对每一个单词给出一个最大后验概率 $\displaystyle\hat{x}_t=\argmax p(Y_t={y_t}|X_t={x_t})p(X_t={x_t})$。但这显然是不合理的，最终只能给出每个单词的所属词性最大概率的一个，但实际上每个单词与前后文相关，而在朴素贝叶斯中却无法体现出来。

为了考虑时序的概率情况，隐马尔科夫模型被提了出来。隐马尔科夫是在马尔科夫链的基础上提出来的，它们之间的不同之处就在于，马尔科夫链中的状态都是可观测的，而隐马尔科夫模型中存在不可观测的状态，但它们之间的根本原理都是无后效性，即未来状态只与当前的状态有关，即
$$P(X_{t+1}=x_{t+1}|X_t=x_t,\cdots,X_0=x_0)=P(X_{t+1}=x_{t+1}|X_t=x_{t})$$

隐马尔科夫模型中存在一个隐藏的马尔科夫链，也就是状态序列 $X=(X_0,X_1,\dots,X_T)$，状态集合为 $\mathcal{X}$，对于任意两个状态 $i,j\in\mathcal{X}$ 都可以给出一个转移概率 

$$a_{ij}=P(X_{t+1}=j|X_{t}=i)\qquad i,j=1,2,\dots,N$$

因此最终可以得到一个状态转移概率矩阵
$$A=\begin{bmatrix}
    a_{11}&a_{12}&\cdots&a_{1N}\\
    a_{21}&a_{22}&\cdots&a_{2N}\\
    \vdots  \\
    a_{N1}&a_{N2}&\cdots&a_{NN}\\
\end{bmatrix}$$

同时存在另一个随机变量序列 $Y=(Y_0,Y_2,\dots,Y_T)$ 称为观测序列，它的取值来自于观测空间 $\mathcal{Y}$，我们假设观测具有独立性，即任意时刻的观测只依赖于该时刻的马尔科夫链的状态，而与其他时刻观测及状态无关。因此，在给定时刻 $t$ 状态 $b_j$ 的情况下，都可以得到对应观测值 $k$ 的概率
$$b_{j}(k)=P(Y_t=k|X_t=j)\qquad k=1,2,\dots,M;j=1,2,\dots,N$$
进一步地可以得到一个观测概率矩阵
$$B=\begin{bmatrix}
    b_{11}&b_{12}&\cdots&b_{1M}\\
    b_{21}&b_{22}&\cdots&b_{2M}\\
    \vdots  \\
    b_{N1}&b_{N2}&\cdots&b_{NM}\\
\end{bmatrix}$$
最后还需要确定初始状态概率 $\pi=(\pi_1,\pi_2,\dots,\pi_N)$，其中
$$\pi_i=P(X_1=i)\qquad i=1,2,\dots,N$$
通过初始状态概率向量 $\pi$、状态转移概率矩阵 $A$，观测概率矩阵 $B$ 就可以确定隐马尔科夫模型，它们也称为隐马尔科夫模型的三要素
$$\lambda=(A,B,\pi)$$

