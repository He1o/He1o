---
title: 10. Hidden Markov Models
date: 2022-04-22
category: Machine learning
---
<!--more-->
# 隐马尔科夫模型
## 1. 历史背景

## 2. 问题引入
假设有一个由多个可观测的样本组合的序列 $\mathbf{x}=(x_1,x_2,\dots,x_T)$，我们称之为观测序列。观测序列中的 $t$ 可以指时间、染色体或 DNA 上位点的索引或单词在句子中的位置，对于每个 $t\in1,2,\dots,T$，我们希望得到一个对应的感兴趣的隐藏状态变量 $z_t\in \mathcal{Z}$，其中 $\mathcal{Z}$ 是有限的集合，由 $z_t$ 组成的序列称为状态序列 $\mathbf{z}$。例如在词性标注中，观测序列就是文档中的 $t$ 个单词，状态序列就是每个单词对应的词性标签。

试想一下，如果用之前的机器学习方法可以怎么做？

可以用贝叶斯估计，最大似然条件概率 $\displaystyle\argmax \frac{p(X=\mathbf{x}|Z=\mathbf{z})p(Z=\mathbf{z})}{p(X=\mathbf{x})}$，显然单词间的组合太多了，需要的样本量之多几乎不可能做到，只能利用朴素贝叶斯的思想，假定 $(x_i,z_i)$ 与 $(x_j,z_j)$ 相互独立，这样我们就可以对每一个单词给出一个最大后验概率 $\displaystyle\hat{z}_t=\argmax p(X_t={x_t}|Z_t={z_t})p(Z_t={z_t})$。但这显然是不合理的，最终只能给出每个单词的所属词性最大概率的一个，但实际上每个单词与前后文相关，而在朴素贝叶斯中却无法体现出来。

为了考虑时序的概率情况，隐马尔科夫模型被提了出来。隐马尔科夫是在马尔科夫链的基础上提出来的，它们之间的不同之处就在于，马尔科夫链中的状态都是可观测的，而隐马尔科夫模型中存在不可观测的状态，但它们之间的根本原理都是无后效性，即未来状态只与当前的状态有关，即
$$P(Z_{t+1}=z_{t+1}|Z_t=z_t,\cdots,Z_0=z_0)=P(Z_{t+1}=z_{t+1}|Z_t=z_{t})$$

隐马尔科夫模型中存在一个隐藏的马尔科夫链，也就是状态序列 $Z=(Z_0,Z_1,\dots,Z_T)$，状态集合为 $\mathcal{Z}$，对于任意两个状态 $i,j\in\mathcal{Z}$ 都可以给出一个转移概率 

$$a_{ij}=P(Z_{t+1}=j|Z_{t}=i)\qquad i,j=1,2,\dots,N$$

因此最终可以得到一个状态转移概率矩阵
$$A=\begin{bmatrix}
    a_{11}&a_{12}&\cdots&a_{1N}\\
    a_{21}&a_{22}&\cdots&a_{2N}\\
    \vdots  \\
    a_{N1}&a_{N2}&\cdots&a_{NN}\\
\end{bmatrix}$$

同时存在另一个随机变量序列 $X=(X_0,X_2,\dots,X_T)$ 称为观测序列，它的取值来自于观测空间 $\mathcal{X}$，我们假设观测具有独立性，即任意时刻的观测只依赖于该时刻的马尔科夫链的状态，而与其他时刻观测及状态无关。因此，在给定时刻 $t$ 状态 $b_j$ 的情况下，都可以得到对应观测值 $k$ 的概率
$$b_{j}(k)=P(X_t=k|Z_t=j)\qquad k=1,2,\dots,M;j=1,2,\dots,N$$
进一步地可以得到一个观测概率矩阵
$$B=\begin{bmatrix}
    b_{11}&b_{12}&\cdots&b_{1M}\\
    b_{21}&b_{22}&\cdots&b_{2M}\\
    \vdots  \\
    b_{N1}&b_{N2}&\cdots&b_{NM}\\
\end{bmatrix}$$
最后还需要确定初始状态概率 $\pi=(\pi_1,\pi_2,\dots,\pi_N)$，其中
$$\pi_i=P(Z_1=i)\qquad i=1,2,\dots,N$$
通过初始状态概率向量 $\pi$、状态转移概率矩阵 $A$，观测概率矩阵 $B$ 就可以确定隐马尔科夫模型，它们也称为隐马尔科夫模型的三要素
$$\lambda=(A,B,\pi)$$

可以发现，隐马尔科夫是基于无后效性和观测独立性假设提出来的。有了三要素，给定序列长度 $T$ 就可以生成一个对应的观测序列，而概率矩阵的拟合根据数据是否标记可采用不同的学习算法。最终想要隐马尔科夫模型进行预测需要解决一下三个问题

1. 学习问题。已知观测序列 $\mathbf{x}=(x_1,x_2,\dots,x_T)$，估计模型 $\lambda=(A,B,\pi)$ 参数，使得在该模型下的观测序列概率 $P(\mathbf{x}|\lambda)$ 最大，也就是用最大似然估计的方法来估计模型参数。
2. 先验概率问题。给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $\mathbf{x}=(x_1,x_2,\dots,x_T)$，计算在模型 $\lambda$ 下观测序列 $\mathbf{x}$ 出现的概率 $P(\mathbf{x}|\lambda)$，也就是先验概率。
3. 预测问题。已知模型 $\lambda=(A,B,\pi)$ 和观测序列 $\mathbf{x}=(x_1,x_2,\dots,x_T)$，求在给定观测序列下条件概率 $P(\mathbf{z}|\mathbf{x})$ 最大的状态序列 $\mathbf{z}=(z_1,z_2,\dots,z_T)$。

## 3. 先验概率计算算法
给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $\mathbf{x}=(x_1,x_2,\dots,x_T)$，计算观测序列 $\mathbf{x}$ 出现的概率 $P(\mathbf{x}|\lambda)$，也就是先验概率。

通过概率公式可以直接计算，通过列举所有长度为 $T$ 可能的状态序列 $\mathbf{z}=(z_1,z_2,\dots,z_T)$，求每个状态序列 $\mathbf{z}$ 与观测序列 $\mathbf{x}$ 的联合概率 $P(\mathbf{x}, \mathbf{z}|\lambda)$，然后对所有可能的状态序列求和，就可以得到先验概率 $P(\mathbf{x}|\lambda)$。




### 3.1 监督学习



## 4. 学习算法
隐马尔科夫模型参数的学习可分为监督学习与无监督学习。
### 4.1 监督学习
在监督学习中，观测序列相当于特征值，状态序列相当于标注结果，假设给出数据集
$$T=\{(\mathbf x_1,\mathbf z_1),(\mathbf x_2,\mathbf z_2),...,(\mathbf x_S,\mathbf z_S)\}$$
其中，观测序列 $\mathbf x_i$ 与状态序列 $\mathbf z_i$ 长度均为 $T$，因此可以通过最大似然估计来确定隐马尔科夫模型的参数。

假设样本中时刻 $t$ 处于状态 $i$ 时刻 $t+1$ 转移到状态 $j$ 的频数为 $c_{ij}$，状态空间的总数为 $N$，那么状态转移概率 $a_{ij}$ 的估计为 
$$ \hat a_{ij}=\frac{c_{ij}}{\displaystyle \sum_{j=1}^{N}c_{ij}}\qquad i,j=1,2,\dots,N$$
设样本状态 $j$ 并观测为 $k$ 的频数是 $d_{ij}$，观测空间的总数为 $M$，那么状态为 $j$ 时观测为 $k$ 的概率估计为
$$\hat{b}_{j}(k)=\frac{d_{ij}}{\displaystyle \sum_{j=1}^{N}d_{ij}}\qquad k=1,2,\dots,M;j=1,2,\dots,N$$

初始状态概率 $\pi_i$ 的估计 $\hat{\pi}_i$ 为 $S$ 个样本中初始状态为 $i$ 的频率。

### 4.2 无监督学习
如果给定训练数据只包含 $S$ 个长度为 $N$ 的观测序列，而缺少状态序列，那么状态序列可以视为不可观测的隐藏变量，隐马尔科夫模型实际上就是一个存在隐藏变量的概率模型
$$P(\mathbf{x}|\lambda)=\sum_{\mathbf{z}}P(\mathbf{x}|\mathbf{z},\lambda)P(\mathbf{z}|\lambda)$$
我们可以使用 EM 算法求解上述概率模型，同时应用在 HMM 中的 EM 算法也称为 Baum-Welch 算法。由之前已经推导出，第 $k+1$ 次迭代只需要最大化下式即可。
$$\argmax_{\lambda} \sum_{\mathbf{z}}{P}(\mathbf{z}|\mathbf{X}, \lambda_k)\ln{P}(\mathbf{X}, \mathbf{z}|\lambda)$$
由隐马尔科夫三要素可以得到完全数据的似然函数为
$$P(\mathbf{x},\mathbf{z}|\lambda)=\pi(z_1)\prod_{t=2}^NA(z_{t}|z_{t-1})\prod_{t=1}^NB(x_t|z_t)$$
相应的对数似然函数为
$$\ln P(\mathbf{x},\mathbf{z}|\lambda)=\ln{\pi(z_1)}+\sum_{t=2}^N\ln A(z_{t}|z_{t-1})+\sum_{t=1}^N\ln B(x_t|z_t)$$
E 步，得到在 $\lambda_k$ 下的上述对数似然函数的期望
$$\begin{aligned}
    E_{\mathbf{z|\mathbf{X},\lambda}}\{\ln\mathcal{P}(\mathbf{X}, \mathbf{z}|\theta)\} &= \sum_{\mathbf{z}}{P}(\mathbf{z}|\mathbf{X}, \lambda_k)\ln{P}(\mathbf{X}, \mathbf{z}|\lambda) \\
    &= \sum_{\mathbf{z}}{P}(\mathbf{z}|\mathbf{X}, \lambda_k)\ln{\pi(z_1)}+\sum_{\mathbf{z}}\sum_{t=2}^N{P}(\mathbf{z}|\mathbf{X}, \lambda_k)\ln A(z_{t}|z_{t-1})\\
    &+\sum_{\mathbf{z}}\sum_{t=2}^N{P}(\mathbf{z}|\mathbf{X}, \lambda_k)\ln B(x_t|z_t)
\end{aligned}$$
M 步，最大化上述期望，可以分别最大化等式中的每一项。

首先最大化初始状态概率 $\pi$




## 参考
> [1. Hidden Markov model - Wikipedia](https://en.wikipedia.org/wiki/Hidden_Markov_model)
> [2. 隐马尔科夫模型（HMM）一基本模型与三个基本问题 - zhihu](https://zhuanlan.zhihu.com/p/26811689)
> [3. Lecture notes: Hidden Markov Models](https://www.stats.ox.ac.uk/~caron/teaching/sb1b/lecturehmm.pdf)
> [4. Lecture 9: Hidden Markov Model](http://faculty.washington.edu/yenchic/18A_stat516/Lec9_HMM.pdf)
> [5. History and Theoretical Basicsof Hidden Markov Models](https://www.intechopen.com/chapters/15369)
> [6. Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/A.pdf)